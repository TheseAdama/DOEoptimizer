% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/SGD.R
\name{SGDoptim}
\alias{SGDoptim}
\title{Stochastic Gradient Descent Optimization for Design of Experiments}
\usage{
SGDoptim(
  fobj,
  D,
  size,
  maxiter = NULL,
  Xinit = NULL,
  a = 1e-04,
  c = 0.01,
  A = NULL,
  gamma = 1/6,
  isMax = TRUE
)
}
\arguments{
\item{fobj}{A function representing the objective function to be optimized. It should accept a matrix as input where each row is a different set of design.}

\item{D}{A numeric matrix with two columns defining the bounds of the design to be optimized.}

\item{size}{An integer specifying the size of the experimental design (number of points).}

\item{maxiter}{An integer specifying the maximum number of iterations (default: 10000).}

\item{Xinit}{An optional numeric matrix specifying the initial design. If \code{NULL}, a design will be generated using Latin Hypercube Sampling (LHS).}

\item{a}{A numeric value for the initial step size (default: 1e-3).}

\item{c}{A numeric value for the coefficient controlling the amplitude of the perturbations (default: 0.01).}

\item{A}{An optional numeric constant used for adjusting the step size in the geometric decay (default: \code{NULL}, computed as 0.1 * \code{maxiter}).}

\item{gamma}{A numeric value between 0 and 1 for the exponent controlling the decay rate of the perturbation amplitude (default: 1/6).}

\item{isMax}{A logical value indicating whether the objective function should be maximized (\code{TRUE}) or minimized (\code{FALSE}).}
}
\value{
A list with optimized design \code{Xopt} and the corresponding objective value \code{Copt}.
}
\description{
Implements a stochastic gradient descent (SGD) optimization algorithm for
optimizing a function in the context of design of experiments (DOE). The algorithm iteratively
updates the design to either maximize or minimize the objective function.
}
\details{
The algorithm uses a stochastic approach to estimate the gradient of the objective function and iteratively updates the design in the direction of the gradient. The step size diminishes over time, following a geometric decay to ensure convergence.
This approach is based on the Simultaneous Perturbation Stochastic Approximation (SPSA) method, which is particularly efficient for high-dimensional optimization problems. The method is well-documented in the following references:
\itemize{
\item Huan, Xun and Marzouk, Youssef M. (2013). Simulation-based optimal Bayesian experimental design for nonlinear systems. \emph{Journal of Computational Physics}, 232, 288-317.
\item Spall, J. C. (1998). An overview of the simultaneous perturbation method for efficient optimization. \emph{Johns Hopkins APL Technical Digest}, 19, 482-492.
\item Kleinman, N. L., Spall, J. C., & Naiman, D. Q. (1999). Simulation-based optimization with stochastic approximation using common random numbers. \emph{Management Science}, 45, 1570-1578.
\item Spall, J. C. (1992). Multivariate stochastic approximation using a simultaneous perturbation gradient approximation. \emph{IEEE Transactions on Automatic Control}, 37, 332-341.
}
}
\examples{
# Example usage of SGDoptim
fobj <- function(X){return(log(det(t(X)\%*\%X)))}
D <- matrix(c(0, 10, 0, 5), ncol = 2, byrow = TRUE)
R <- SGDoptim(fobj, D, size=10, maxiter=1000)
print(R$Xopt)
print(R$Copt)

}
